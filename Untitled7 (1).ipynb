{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyibSiFptQUt"
      },
      "outputs": [],
      "source": [
        "# 1. Can we use Bagging for regression problems?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, bagging can be used for regression problems. A popular example is the Bagging Regressor, where base models like decision trees are trained on different subsets of the data, and predictions are averaged to get the final output. This helps in reducing variance and improving prediction accuracy for regression tasks."
      ],
      "metadata": {
        "id": "uushnXgvuaPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. What is the difference between multiple model training and single model training?"
      ],
      "metadata": {
        "id": "JN_nit3ttY0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Model Training: Involves training a single model (like a decision tree) on the entire dataset. It may be prone to overfitting or underfitting, especially if the model is either too complex or too simple for the data.\n",
        "\n",
        "Multiple Model Training: Involves training multiple models (like in ensemble methods such as Bagging, Boosting, or Random Forest) on different subsets of data or using different algorithms. The final prediction is often the average or majority vote of all models. This reduces variance, improves robustness, and often yields better performance than a single model."
      ],
      "metadata": {
        "id": "lswzFu0Tubc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Explain the concept of feature randomness in Random Forest."
      ],
      "metadata": {
        "id": "pZzrEkSxtYwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Random Forest, feature randomness refers to the fact that at each split in a decision tree, only a random subset of features is considered, rather than all the features. This feature randomness (or random feature selection) helps make the trees less correlated and prevents overfitting, as each tree has its own unique perspective on the data."
      ],
      "metadata": {
        "id": "a8yaUBorugDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. What is OOB (Out-of-Bag) Score?"
      ],
      "metadata": {
        "id": "gDMSFbFDtYuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Out-of-Bag (OOB) score is an internal validation method used in ensemble learning methods like Bagging and Random Forest. It works by using bootstrap sampling to train the model, where each model is trained on a subset of the data. The remaining data points (those not included in the training subset) are called \"out-of-bag\" samples. These OOB samples are used to evaluate model performance without needing a separate validation set."
      ],
      "metadata": {
        "id": "5NS9Jcmkuj37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. How can you measure the importance of features in a Random Forest model?"
      ],
      "metadata": {
        "id": "Kcy4XWONtYrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Random Forest, feature importance is measured by how much a feature improves the purity of splits across the forest. The two common ways to measure feature importance are:\n",
        "\n",
        "Gini Importance: Calculated by how much the Gini impurity is reduced by each feature in the splits.\n",
        "Permutation Importance: Involves randomly shuffling each feature and measuring how much the model's performance degrades. A more significant performance drop indicates a more important feature."
      ],
      "metadata": {
        "id": "0bfil-zfunEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Explain the working principle of a Bagging Classifier."
      ],
      "metadata": {
        "id": "fm_nv7u1tYpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Bagging Classifier works by:\n",
        "\n",
        "Creating multiple models (usually decision trees) using bootstrap samples from the training data.\n",
        "Each bootstrap sample is a random sample (with replacement) of the training set.\n",
        "These models are trained independently.\n",
        "The final prediction is made by combining the predictions from all models, usually via majority voting (for classification tasks)."
      ],
      "metadata": {
        "id": "KAExEo6vuqsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. How do you evaluate a Bagging Classifierâ€™s performance?"
      ],
      "metadata": {
        "id": "dOj8UfmTtYmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: Check the accuracy using cross-validation or a test set.\n",
        "\n",
        "Out-of-Bag (OOB) Score: Evaluate performance on the OOB samples, which act as a form of cross-validation.\n",
        "\n",
        "Confusion Matrix: To understand the distribution of predictions.\n",
        "\n",
        "ROC Curve and AUC Score: For classification problems, to evaluate performance across different thresholds."
      ],
      "metadata": {
        "id": "xMhSyiAMuvUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. How does a Bagging Regressor work?"
      ],
      "metadata": {
        "id": "OclMqJzqtYjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to a Bagging Classifier, but instead of voting, the final prediction is the average of predictions from all base models (like decision trees). Each model is trained on a different bootstrap sample, and the ensemble helps reduce the variance of predictions."
      ],
      "metadata": {
        "id": "DoZE2zDFu134"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. What is the main advantage of ensemble techniques?"
      ],
      "metadata": {
        "id": "8k2vccsztYhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increased Accuracy: By combining the predictions of multiple models, ensemble methods often result in better generalization and higher accuracy compared to single models.\n",
        "\n",
        "Reduced Overfitting: Methods like Bagging (Random Forest) reduce overfitting by averaging multiple models, each of which sees a different aspect of the data."
      ],
      "metadata": {
        "id": "i3dCkDU5u4zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. What is the main challenge of ensemble methods?"
      ],
      "metadata": {
        "id": "y9p_5Y5gtYef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complexity: Training and maintaining multiple models is computationally expensive.\n",
        "\n",
        "Interpretability: Ensemble methods, like Random Forests, are harder to interpret compared to single decision trees or simpler models."
      ],
      "metadata": {
        "id": "ZZ6KGnpovAFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Explain the key idea behind ensemble techniques."
      ],
      "metadata": {
        "id": "kXPUVP_RtYbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques rely on combining the predictions of multiple models to improve overall performance. The idea is that multiple weak models (models that perform slightly better than random guessing) can be combined to form a stronger model by averaging or voting on their predictions."
      ],
      "metadata": {
        "id": "ZHTO5OmSvEcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. What is a Random Forest Classifier?"
      ],
      "metadata": {
        "id": "2Oubfl_ftYY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Random Forest Classifier is an ensemble learning method that builds multiple decision trees (on random subsets of data and features) and combines their predictions (using majority voting for classification tasks). It reduces overfitting and improves accuracy by leveraging the diversity of decision trees."
      ],
      "metadata": {
        "id": "kV-NF-iPvKn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. What are the main types of ensemble techniques?"
      ],
      "metadata": {
        "id": "qeU3FOdytYWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging (Bootstrap Aggregating): Reduces variance by training multiple models on different bootstrap samples of the data.\n",
        "\n",
        "Boosting: Sequentially trains models where each model tries to correct the errors made by the previous one.\n",
        "\n",
        "Stacking: Combines multiple models by using another model (meta-model) to learn from their predictions."
      ],
      "metadata": {
        "id": "PysO696FvP8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. What is ensemble learning in machine learning?"
      ],
      "metadata": {
        "id": "-_yO-gxGtYTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning is a machine learning technique that combines multiple base models to improve the accuracy and robustness of predictions. The base models could be of the same type (e.g., decision trees) or different types (e.g., decision trees and SVMs)."
      ],
      "metadata": {
        "id": "zF8DFZcdvTXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. When should we avoid using ensemble methods?"
      ],
      "metadata": {
        "id": "KbK4dWUdtYRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small Datasets: Ensemble methods like Random Forests or Boosting may not perform well on small datasets due to overfitting.\n",
        "\n",
        "High Complexity: If interpretability is critical, simpler models may be preferable, as ensemble methods tend to be more complex and harder to interpret."
      ],
      "metadata": {
        "id": "tUXjrRhfva5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. How does Bagging help in reducing overfitting?"
      ],
      "metadata": {
        "id": "ADVn2jT5tYNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging helps reduce overfitting by training multiple models on different subsets of the data. Each model may overfit its own subset, but by averaging their predictions, the overall variance is reduced, and the model becomes more generalizable."
      ],
      "metadata": {
        "id": "RaYWJgeXvddL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Why is Random Forest better than a single Decision Tree?"
      ],
      "metadata": {
        "id": "aBw2U-C9tYGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest reduces overfitting compared to a single decision tree by averaging multiple trees, each trained on different random subsets of data and features. This leads to a more robust and accurate model."
      ],
      "metadata": {
        "id": "-78FUAq0vhC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. What is the role of bootstrap sampling in Bagging?"
      ],
      "metadata": {
        "id": "c-3uuROmuLLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstrap sampling creates different subsets of data (by sampling with replacement) for training each base model. This ensures that each model sees a slightly different version of the dataset, which helps in reducing variance and preventing overfitting."
      ],
      "metadata": {
        "id": "X2UoFVXPvkce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. What are some real-world applications of ensemble techniques?"
      ],
      "metadata": {
        "id": "wC-rYm59uLAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fraud Detection: Ensemble methods like Random Forests are commonly used in financial fraud detection systems.\n",
        "\n",
        "Customer Churn Prediction: Companies use ensemble techniques to predict which customers are likely to leave.\n",
        "\n",
        "Medical Diagnosis: Ensemble methods improve the accuracy of models predicting diseases or medical conditions.\n",
        "\n",
        "Recommendation Systems: Ensemble techniques help in building better recommender systems by combining predictions from multiple algorithms."
      ],
      "metadata": {
        "id": "yLQdvt7_vnOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "U-Y-qIk5uK6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Bagging: Models are trained independently in parallel on different subsets of data, and their predictions are averaged. The goal is to reduce variance.\n",
        "\n",
        "Boosting: Models are trained sequentially, with each new model focusing on correcting the errors of the previous one. The goal is to reduce bias and improve accuracy."
      ],
      "metadata": {
        "id": "2o_AVPWZvtfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                            # Practical"
      ],
      "metadata": {
        "id": "96RkLghMvyol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "1gwP4Pd5v46N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "cxa1MUl6y8BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "54tY1_DDwJhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = make_regression(n_samples=100, n_features=4, noise=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate MSE\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "R5J6Ipc8y9KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "_95BhRtRwJeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = pd.Series(rf_clf.feature_importances_, index=data.feature_names)\n",
        "print(feature_importances.sort_values(ascending=False).head())\n"
      ],
      "metadata": {
        "id": "I5SywyYEzBoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "8y1AokOkwJbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(X_train, y_train)\n",
        "y_pred_tree = tree_reg.predict(X_test)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_tree)}\")\n",
        "print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n"
      ],
      "metadata": {
        "id": "39Iib_kCzEsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "OxmUSm0xwJYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"OOB Score: {rf_clf.oob_score_}\")\n"
      ],
      "metadata": {
        "id": "DS7Q37L8zHYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "ZRVPBGMuwJVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train Bagging Classifier with SVM\n",
        "bagging_svc = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_svc.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "y_pred = bagging_svc.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "NQd2N_JVzKCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "fvL_N0NcwJSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for n_estimators in [10, 50, 100]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    print(f\"Accuracy with {n_estimators} trees: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "BMbO5thpzNgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "N6ueRK0HwJQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Train Bagging Classifier with Logistic Regression\n",
        "bagging_logreg = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10, random_state=42)\n",
        "bagging_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print AUC score\n",
        "y_pred_proba = bagging_logreg.predict_proba(X_test)[:, 1]\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba)}\")\n"
      ],
      "metadata": {
        "id": "Mtfop5iDzS3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "NFZmd-41wJNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "importances = rf_reg.feature_importances_\n",
        "print(importances)\n"
      ],
      "metadata": {
        "id": "HWH9pQcXzVjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Train an ensemble model using both Bagging and Random Forest and compare accuracy"
      ],
      "metadata": {
        "id": "mWw9nb99wJKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Compare accuracy\n",
        "print(f\"Bagging Accuracy: {accuracy_score(y_test, bagging_pred)}\")\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred)}\")\n"
      ],
      "metadata": {
        "id": "IlBHeujAzYSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "YzcK1ZX1wJHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Load dataset and train model\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters and accuracy\n",
        "print(f\"Best Params: {grid_search.best_params_}\")\n",
        "print(f\"Best Score: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "id": "jfIBWpRmzcSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "5ctMwGWPwJEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Try different numbers of base estimators\n",
        "for n_estimators in [10, 50, 100]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with {n_estimators} estimators: {mse}\")\n"
      ],
      "metadata": {
        "id": "_qluPowLzgDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "jH3jOxdowJB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified = X_test[y_test != y_pred]\n",
        "print(f\"Misclassified samples: {misclassified}\")\n"
      ],
      "metadata": {
        "id": "pTzOiqYAzjsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "uLA_CmCpwI-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Decision Tree Classifier\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "tree_pred = tree_clf.predict(X_test)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Compare accuracy\n",
        "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, tree_pred)}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_score(y_test, bagging_pred)}\")\n"
      ],
      "metadata": {
        "id": "F2fLQgt9zm8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "n5GcbmqqwI7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(rf_clf, X_test, y_test)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1VslF3BxzpzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "JZ5fmUVdwI4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svc', SVC(probability=True)),\n",
        "    ('lr', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compare accuracy\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "P-Y-Ynh2zslD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "RP6s17xjwI1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf_clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print top 5 most important features\n",
        "for i in range(5):\n",
        "    print(f\"Feature {i + 1}: {X_train.columns[indices[i]]} (Importance: {importances[indices[i]]})\")\n"
      ],
      "metadata": {
        "id": "0aorhNgjzvJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "FvRg--phwIzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate using Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "DyYhvDehzxlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "J-twPiOIwIvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for max_depth in [5, 10, 15, 20]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    print(f\"Accuracy with max_depth={max_depth}: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "OQkkhpxez1Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
      ],
      "metadata": {
        "id": "0flPX57OwIsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Train Bagging Regressor with DecisionTree\n",
        "bagging_dt = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "dt_pred = bagging_dt.predict(X_test)\n",
        "\n",
        "# Train Bagging Regressor with KNeighbors\n",
        "bagging_knn = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "knn_pred = bagging_knn.predict(X_test)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Decision Tree MSE: {mean_squared_error(y_test, dt_pred)}\")\n",
        "print(f\"KNeighbors MSE: {mean_squared_error(y_test, knn_pred)}\")\n"
      ],
      "metadata": {
        "id": "3hK-jVwlz41S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "8H6UmTcMwIp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and evaluate ROC-AUC\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n"
      ],
      "metadata": {
        "id": "G0pwiLj8z8KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Train a Bagging Classifier and evaluate its performance using cross-validation"
      ],
      "metadata": {
        "id": "d7C6xg--wInb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
        "\n",
        "# Print cross-validation performance\n",
        "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
        "print(f\"Mean CV Score: {cv_scores.mean()}\")\n"
      ],
      "metadata": {
        "id": "2FvQw7sVz-6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "ujlq89_7wIkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h8cIEXSb0E4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "z3FljJRxwIim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compare accuracy\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "sk7vqE3b0JKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
      ],
      "metadata": {
        "id": "i8UDwjYCwIeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for bootstrap_samples in [0.5, 0.7, 1.0]:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, bootstrap_samples=bootstrap_samples, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with bootstrap_samples={bootstrap_samples}: {mse}\")\n"
      ],
      "metadata": {
        "id": "q3MQD_Xg0kdv"
      }
    }
  ]
}